#+OPTIONS: toc:nil
#+HTML_HEAD: <link href="../css/solarized-dark.css" rel="stylesheet" />
#+HTML_LINK_HOME: ../index.html
#+TITLE: Toy Domain Scraper
2016-04-24

Scraping linked pages within a given domain can be thought of as traversing a graph: the links are edges and the individual pages are nodes. For this project, I'm using [[https://en.wikipedia.org/wiki/Breadth-first_search][breadth-first traversal]]. (It's worth noting that in breadth-first traversal, you do not necessarily need to keep track of which nodes have already been visited. In this case, we do, because we have to assume that there will be backwards-pointing links from child nodes. In depth-first traversal, you always keep track because you are constantly climbing back up the tree towards the root.) Ideally, the script visits every node, correctly identifying previously visited nodes to avoid infinite loops, and it does so in a reasonable amount of time (whatever that might be). In that scenario it might not matter what type of traversal you used. If the script does get tangled in a loop, however, or if it can't finish traversing quickly, the breadth-first traversal is going to produce a good set of results right away for a typical website. This is why I've included the ~try...finally~ block in ~scrape()~; in the event that you need to manually end the script, you can still see which nodes were visited.

Building absolute urls out of relative urls turned out to be pretty involved. What I've written covers individual cases I encountered, as generally as possible. It isn't airtight, though, so I left in the print functions for debugging. (There may be a library out there that already does this, maybe something like os.path but for urls.)

Also: Regex syntax is dense! I wish I knew of a way to make it more expressive. Reading over the expressions a week after writing them, I find myself working quite hard to decipher them.

Lastly, you'll appreciate that I wrote ~get_links~ as a generator to save space in memory; I'm using a deque to save on ~.pop()~ and ~.appendleft()~; and that I'm using a set to save on testing membership, no matter the number of individual pages.

See the code below; project is also on [[https://github.com/mastokley/toy_domain_scraper][github]].[fn:1]

#+BEGIN_SRC python -n
from __future__ import print_function
import io
import os
import re
import requests
import sys
from bs4 import BeautifulSoup
from collections import deque
from time import sleep


def build_absolute_url(base, extension):
    """Return an absolute url given a base url and a relative url."""
    print('\nbase:\t\t{}'.format(base))
    print('extension:\t{}'.format(extension))
    if extension == '/':
        url = base
    elif extension[:3] == '../':
        # safe to mutate base, extension
        base = re.sub(r'[^/]*$', '', base)
        while extension[:3] == r'../':
            extension = extension[3:]
            base = re.sub(r'[^/]*/$', '', base)
        url = ''.join([base, extension])
    elif extension[:2] == '//':
        base = re.sub(r':.*$', ':', base)
        url = ''.join([base, extension])
    elif extension[0] != r'/':
        url = re.sub(r'[^/]*\.[^/]*$', extension, base)
    else:
        url = '/'.join([base, extension])
    url = re.sub(r'(?<!\:)/{2,}', '/', url)  # remove extra forward slashes
    print('url:\t\t{}'.format(url))
    return url


def get_links(response, url):
    """Yield links on page for given url."""
    soup = BeautifulSoup(response.content, 'html.parser')
    a_tags = soup.find_all('a', href=True)
    for a_tag in a_tags:
        link = a_tag['href']
        if 'http' in link and url in link and '#' not in link:
            yield link
        elif 'http' not in link and '/' in link and '#' not in link:
            absolute_link = build_absolute_url(response.url, link)
            yield absolute_link


def write_to_file(response, url):
    """Write vanilla html file to disk for given url."""
    # TODO: deal with url forward slashes better
    # TODO: write filenames such that links in response.content work
    # TODO: write files into a directory structure (not flatly)
    filename = ''.join(['../data/',
                        url.replace(r'/', '_'),
                        '.html'])
    if os.path.isfile(filename):
        print('File found: {}'.format(url))
    good_response = response.status_code // 100 in set([2, 3])
    if good_response:
        with io.open(filename, 'wb') as fh:
            for chunk in response.iter_content(8192):
                fh.write(chunk)
        print('Wrote {}'.format(filename))


def scrape(root_url):
    """Write html file for all pages containing given root url.

    Uses breadth first traversal."""
    queue = deque()
    visited = set()  # order unimportant
    queue.appendleft(root_url)
    # TODO: determine depth from root for given node
    try:
        while queue:
            url = queue.pop()
            response = requests.get(url)
            write_to_file(response, url)
            for link in get_links(response, url):
                if link not in visited:
                    visited.add(link)
                    queue.appendleft(link)
    finally:
        print(visited)


if __name__ == '__main__':
    scrape(*sys.argv[1])
#+END_SRC

[fn:1] I attempted to rewrite the above using fewer French/latinate words, for fun. Many of the latinate words are technical jargon that can't easily be replaced. (For example: 'traversal', 'generator', 'absolute', 'relative', 'script', 'library' referring to a module, 'pages' referring to webpages, 'code', 'memory'.) The Germans weren't known for their books, I guess.
#+BEGIN_QUOTE
Scraping linked pages within a given domain can be thought of as walking a graph: the links are edges and the pages are nodes. Here, I'm using [[https://en.wikipedia.org/wiki/Breadth-first_search][breadth-first traversal]]. Ideally, the script visits every node once, skirting already visited nodes to avoid endless loops, and it does so quickly. In that scenario it might not matter how you choose to walk the graph. If the script does get tangled in a loop, however, or if it can't finish walking the graph quickly, breadth-first is going to yield a good set of results right away for a typical website. This is why I've used the ~try...finally~ block in ~scrape()~; in the event that you need to end the script by hand, you can still see which nodes were visited.

Building absolute urls out of relative urls ended up being pretty hard. What I've written works for each case I came across, as generally it can. It's brittle, though, so I left in the print functions for debugging. (There may be a library out there that already does this, maybe something like os.path but for urls.)

Also: Regex syntax is... murky. I wish I knew how to make it easier to read. Reading over the expressions a week after writing them, I find them hard to understand.

Lastly, you'll like that I wrote ~get_links~ as a generator to save space in memory; I'm using a deque to save on ~.pop()~ and ~.appendleft()~; and that I'm using a set to save on testing membership, no matter how many pages you end up with.

See the code below; work is also on [[https://github.com/mastokley/toy_domain_scraper][github]].
#+END_QUOTE
